from meta_mb.samplers.base import SampleProcessorfrom meta_mb.utils import utilsimport numpy as npclass ModelBaseSampler(SampleProcessor):    def process_samples(self, paths, log=False, log_prefix=''):        """        Processes sampled paths. This involves:            - computing discounted rewards (returns)            - fitting baseline estimator using the path returns and predicting the return baselines            - estimating the advantages using GAE (+ advantage normalization id desired)            - stacking the path data            - logging statistics of the paths        Args:            paths_meta_batch (dict): A list of dict of lists, size: [meta_batch_size] x (batch_size) x [5] x (max_path_length)            log (boolean): indicates whether to log            log_prefix (str): prefix for the logging keys        Returns:            (list of dicts) : Processed sample data among the meta-batch; size: [meta_batch_size] x [7] x (batch_size x max_path_length)        """        assert self.baseline, 'baseline must be specified'        samples_data, paths = self._compute_samples_data(paths)        # 8) log statistics if desired        self._log_path_stats(paths, log=log, log_prefix=log_prefix)        return samples_data    def _compute_samples_data(self, paths):        assert type(paths) == list        # 1) compute discounted rewards (returns)        for idx, path in enumerate(paths):            path["returns"] = utils.discount_cumsum(path["rewards"], self.discount)        # 2) fit baseline estimator using the path returns and predict the return baselines        self.baseline.fit(paths, target_key="returns")        all_path_baselines = [self.baseline.predict(path) for path in paths]        # 3) compute advantages and adjusted rewards        paths = self._compute_advantages(paths, all_path_baselines)        # 4) stack path data        observations, next_observations, actions, rewards, dones, returns, advantages, env_infos, agent_infos = \            self._concatenate_path_data(paths)        # 5) if desired normalize / shift advantages        if self.normalize_adv:            advantages = utils.normalize_advantages(advantages)        if self.positive_adv:            advantages = utils.shift_advantages_to_positive(advantages)        # 6) create samples_data object        samples_data = dict(            observations=observations,            next_observations=next_observations,            actions=actions,            rewards=rewards,            dones=dones,            returns=returns,            advantages=advantages,            env_infos=env_infos,            agent_infos=agent_infos,        )        return samples_data, paths    def _concatenate_path_data(self, paths):        observations = np.concatenate([path["observations"][:-1] for path in paths])        next_observations = np.concatenate([path["observations"][1:] for path in paths])        actions = np.concatenate([path["actions"][:-1] for path in paths])        rewards = np.concatenate([path["rewards"] for path in paths])        dones = np.concatenate([path["dones"] for path in paths])        returns = np.concatenate([path["returns"] for path in paths])        advantages = np.concatenate([path["advantages"] for path in paths])        env_infos = utils.concat_tensor_dict_list([path["env_infos"] for path in paths])        agent_infos = utils.concat_tensor_dict_list([path["agent_infos"] for path in paths])        return observations, next_observations, actions, rewards, dones, returns, advantages, env_infos, agent_infos