from meta_mb.samplers.base import SampleProcessorfrom meta_mb.utils import utilsimport numpy as npclass ModelSampleProcessor(SampleProcessor):    def __init__(            self,            baseline=None,            discount=0.99,            gae_lambda=1,            normalize_adv=False,            positive_adv=False,    ):        self.baseline = baseline        self.discount = discount        self.gae_lambda = gae_lambda        self.normalize_adv = normalize_adv        self.positive_adv = positive_adv    def process_samples(self, paths, log=False, log_prefix=''):        """        Processes sampled paths. This involves:            - computing discounted rewards (returns)            - fitting baseline estimator using the path returns and predicting the return baselines            - estimating the advantages using GAE (+ advantage normalization id desired)            - stacking the path data            - logging statistics of the paths        Args:            paths_meta_batch (dict): A list of dict of lists, size: [meta_batch_size] x (batch_size) x [5] x (max_path_length)            log (boolean): indicates whether to log            log_prefix (str): prefix for the logging keys        Returns:            (list of dicts) : Processed sample data among the meta-batch; size: [meta_batch_size] x [7] x (batch_size x max_path_length)        """        samples_data, paths = self._compute_samples_data(paths)        # 8) log statistics if desired        self._log_path_stats(paths, log=log, log_prefix=log_prefix)        return samples_data    def _compute_samples_data(self, paths):        assert type(paths) == list        # 1) compute discounted rewards (returns)        for idx, path in enumerate(paths):            path["returns"] = utils.discount_cumsum(path["rewards"], self.discount)        # 4) stack path data        observations, next_observations, actions, rewards, dones, returns, time_steps, env_infos, agent_infos = \            self._concatenate_path_data(paths)        # 6) create samples_data object        samples_data = dict(            observations=observations,            next_observations=next_observations,            actions=actions,            rewards=rewards,            dones=dones,            returns=returns,            advantages=returns, # FIXME: Hack for SVG            time_steps=time_steps,            env_infos=env_infos,            agent_infos=agent_infos,        )        return samples_data, paths    def _concatenate_path_data(self, paths):        observations = np.concatenate([path["observations"][:-1] for path in paths])        next_observations = np.concatenate([path["observations"][1:] for path in paths])        actions = np.concatenate([path["actions"][:-1] for path in paths])        rewards = np.concatenate([path["rewards"][:-1] for path in paths])        dones = np.concatenate([path["dones"][:-1] for path in paths])        returns = np.concatenate([path["returns"][:-1] for path in paths])        time_steps = np.concatenate([np.arange(len(path["observations"][:-1])) for path in paths])        env_infos = utils.concat_tensor_dict_list([path["env_infos"] for path in paths], end=-1)        agent_infos = utils.concat_tensor_dict_list([path["agent_infos"]for path in paths], end=-1)        return observations, next_observations, actions, rewards, dones, returns, time_steps, env_infos, agent_infos