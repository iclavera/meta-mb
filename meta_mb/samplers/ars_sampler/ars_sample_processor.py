from meta_mb.samplers.base import SampleProcessorfrom meta_mb.utils import utilsfrom meta_mb.logger import loggerimport numpy as npclass ARSSamplerProcessor(SampleProcessor):    def __init__(            self,            baseline=None,            discount=0.99,            gae_lambda=1,            normalize_adv=True,            positive_adv=False,            uncertainty_coeff=0.    ):        self.baseline = baseline        self.discount = discount        self.gae_lambda = gae_lambda        self.normalize_adv = normalize_adv        self.positive_adv = positive_adv        self.uncertainty_coeff = uncertainty_coeff    def process_samples(self, samples_data, returns_type=None, log=False, log_prefix=''):        """        Processes sampled paths. This involves:            - computing discounted rewards (returns)            - fitting baseline estimator using the path returns and predicting the return baselines            - estimating the advantages using GAE (+ advantage normalization id desired)            - stacking the path data            - logging statistics of the paths        Args:            samples_data (dict): A list of dict of numpy arrays of size (horizon, num_deltas, batch_size, dim)            log (boolean): indicates whether to log            log_prefix (str): prefix for the logging keys        Returns:            (list of dicts) : Processed sample data among the meta-batch; size: [meta_batch_size] x [7] x (batch_size x max_path_length)        """        samples_data = self._compute_samples_data(samples_data, returns_type)        # 8) log statistics if desired        # TODO: Add the statistics        self._log_path_stats(samples_data, log=log, log_prefix=log_prefix)        return samples_data    def _compute_samples_data(self, samples_data, returns_type):        # returns = np.mean(samples_data['returns'], axis=-1)        if returns_type is None:            returns = samples_data['returns']        elif returns_type == 'mean':            returns = np.sum(np.mean(samples_data['rewards'], axis=-1) - self.uncertainty_coeff * np.std(samples_data['rewards'], axis=-1),                         axis=0)        # 6) create samples_data object        samples_data = dict(            observations=samples_data['observations'],            actions=samples_data['actions'],            rewards=samples_data['rewards'],            dones=samples_data['dones'],            returns=returns,        )        return samples_data    def _log_path_stats(self, samples_data, log=False, log_prefix=''):        # compute log stats        # average_discounted_return = np.mean([path["returns"][0] for path in paths])        # undiscounted_returns = [sum(path["rewards"]) for path in paths]        logger.logkv(log_prefix + 'AverageReturn', np.mean(samples_data['returns']))        # elif log == 'all' or log is True:        #     logger.logkv(log_prefix + 'AverageDiscountedReturn', average_discounted_return)        #     logger.logkv(log_prefix + 'AverageReturn', np.mean(undiscounted_returns))        #     logger.logkv(log_prefix + 'NumTrajs', len(paths))        #     logger.logkv(log_prefix + 'StdReturn', np.std(undiscounted_returns))        #     logger.logkv(log_prefix + 'MaxReturn', np.max(undiscounted_returns))        #     logger.logkv(log_prefix + 'MinReturn', np.min(undiscounted_returns))